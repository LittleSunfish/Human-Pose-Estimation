{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coco_json_path = '/home/sojeong/CV/deep-high-resolution-net.pytorch/data/coco/annotations/person_keypoints_train2017.json'\n",
    "train_coco_img_path  = '/home/sojeong/CV/deep-high-resolution-net.pytorch/data/coco/images/train2017/'\n",
    "test_coco_json_path = '/home/sojeong/CV/deep-high-resolution-net.pytorch/data/coco/annotations/person_keypoints_val2017.json'\n",
    "test_coco_img_path = '/home/sojeong/CV/deep-high-resolution-net.pytorch/data/coco/images/val2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the cropped COCO images\n",
    "class CustomCOCO(Dataset):\n",
    "\n",
    "    def __init__(self, json_path, img_path):\n",
    "        data = json.load(open(json_path))\n",
    "        self.json_path = json_path\n",
    "        self.anot = data['annotations']\n",
    "\n",
    "        #separate image_ids, keypoints, and bbox from data\n",
    "        self.img_path  = img_path\n",
    "        self.target_height = 256\n",
    "        self.target_width = 192\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anot)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.load_img(idx)\n",
    "        img = self.resized_image\n",
    "        hp = np.array(self.create_heatmap())\n",
    "        # tenrsor size\n",
    "        return torch.Tensor(img).permute(2,0,1), torch.Tensor(hp), torch.Tensor(self.keypoint_confidence)\n",
    "    \n",
    "    def get_id(self, index):\n",
    "        self._target_image_info(index)\n",
    "        return self.target_id\n",
    "    \n",
    "    def _target_image_info(self, index):\n",
    "        target_data = self.anot[index]\n",
    "        self.target_kp = target_data['keypoints']\n",
    "        self.target_bbox = target_data['bbox']\n",
    "        self.target_id = target_data['image_id']\n",
    "\n",
    "    def _crop_img(self):\n",
    "        X,Y,width,height = self.target_bbox\n",
    "        X,Y,width,height = int(X),int(Y), int(width)+1, int(height)+1\n",
    "\n",
    "        self.padding_up, padding_down, self.padding_left, padding_right = 0,0,0,0\n",
    "\n",
    "        if height/width < 4/3:\n",
    "            original_height = height\n",
    "            height = int(3*width/4)\n",
    "            height_change = height - original_height\n",
    "            ## 5% padding for robustness\n",
    "            original_width = width\n",
    "            width = int(width * 1.05)\n",
    "            width_change = width - original_width\n",
    "\n",
    "        else:\n",
    "            original_width = width\n",
    "            width = int(3*height/4)\n",
    "            width_change = width - original_width\n",
    "            ## 5% padding for robustness\n",
    "            original_height = height\n",
    "            height = int(height * 1.05)\n",
    "            height_change = height - original_height\n",
    "\n",
    "        Y_start = Y-int(height_change/2)\n",
    "        Y_end   = Y+height-int(height_change/2)\n",
    "        X_start = X-int(width_change/2)\n",
    "        X_end   = X-int(width_change/2)+width\n",
    "\n",
    "        #print(\"X and Y\", Y_start, Y_end, X_start, X_end)\n",
    "\n",
    "        \n",
    "        if Y_start<0 or X_start <0:\n",
    "            # apply padding -> up and left\n",
    "            img_size = self.target_image.shape\n",
    "            #print(\"img_size \", img_size)\n",
    "            pad = min(Y-int(height_change/2), X-int(width_change/2)) # pick more negative one\n",
    "            pad = abs(pad)+20\n",
    "            Y_start += pad; X_start += pad; Y_end += pad; X_end += pad\n",
    "            self.padding_up += pad; self.padding_left += pad\n",
    "            #print(\"pad: \", pad)\n",
    "            zero_array = np.zeros((img_size[0] + pad, img_size[1]+ pad, 3))\n",
    "            zero_array[pad:img_size[0]+pad, pad:img_size[1]+pad, :] = self.target_image/255.0\n",
    "            self.target_image = zero_array\n",
    "\n",
    "        img_size = self.target_image.shape\n",
    "        if Y_end > img_size[0] or X_end > img_size[1]:\n",
    "            # apply padding -> down and right\n",
    "            pad = max(Y_end-img_size[0], X_end-img_size[1])\n",
    "            pad = abs(pad)+20\n",
    "            Y_end += pad; X_end += pad\n",
    "            zero_array = np.zeros((img_size[0] + pad, img_size[1]+ pad, 3))\n",
    "            zero_array[:img_size[0], :img_size[1], :] = self.target_image/255.0\n",
    "            self.target_image = zero_array\n",
    "\n",
    "        #print(\"right before cropping X and Y\", Y_start, Y_end, X_start, X_end)\n",
    "        self.cropped_image = self.target_image[Y_start:Y_end, X_start:X_end, :]\n",
    "        #print(\"after cropping \", self.cropped_image.shape)\n",
    "\n",
    "        # if Y-int(height_change/2) < 0:\n",
    "        #     self.cropped_image = self.target_image[1:Y+height-int(height_change/2),X-int(width_change/2):X-int(width_change/2)+width]\n",
    "        \n",
    "        self.width = width; self.height = height\n",
    "        self.width_trans = X - int(width_change/2)\n",
    "        self.height_trans = Y - int(height_change/2)\n",
    "\n",
    "    def _resize_img(self):\n",
    "        dim = (self.target_width, self.target_height)\n",
    "        #print(\"img size before resizing: \", self.cropped_image.shape)\n",
    "        self.resized_image = cv2.resize(self.cropped_image, dim, interpolation=cv2.INTER_AREA)\n",
    "        #print(torch.Tensor(self.resized_image).shape)\n",
    "\n",
    "    def load_img(self, index):\n",
    "        self._target_image_info(index)\n",
    "        full_target_id = str(self.target_id).rjust(12, '0')\n",
    "        target_img_path = self.img_path + full_target_id + '.jpg'\n",
    "        #print(\"id: \", full_target_id)\n",
    "        self.target_image = cv2.imread(target_img_path)\n",
    "        self.target_image = cv2.cvtColor(self.target_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # apply transformation\n",
    "        #print(\"before crop: \", self.target_image.shape)\n",
    "        self._crop_img()\n",
    "        #print(\"before resize: \", self.cropped_image.shape)\n",
    "        self._resize_img()\n",
    "\n",
    "    # Only for Debugging\n",
    "    def show_original(self):\n",
    "        plt.imshow(self.target_image)\n",
    "        plt.show()\n",
    "\n",
    "    def show_original_bbox(self):\n",
    "        X,Y,width,height = self.target_bbox\n",
    "        X,Y,width,height = int(X),int(Y), int(width)+1, int(height)+1\n",
    "        plt.imshow(self.target_image)\n",
    "        ax = plt.gca()\n",
    "        rect = patches.Rectangle((X,Y), width, height, linewidth=2, edgecolor='cyan', fill = False)\n",
    "        ax.add_patch(rect)\n",
    "        plt.show()\n",
    "\n",
    "    def show_img(self):\n",
    "        plt.imshow(self.resized_image)\n",
    "        plt.show()\n",
    "\n",
    "    def _create_kp_list(self):\n",
    "        # execute after load_img() \n",
    "        self.keypoint = []\n",
    "        self.keypoint_confidence = []\n",
    "        for i, k in enumerate(self.target_kp):\n",
    "            if i%3 != 0:\n",
    "                continue\n",
    "\n",
    "            x = self.target_kp[i] #k\n",
    "            y = self.target_kp[i+1]\n",
    "            confidence = self.target_kp[i+2]\n",
    "            \n",
    "            if confidence != 0:\n",
    "                x = (x-self.width_trans+self.padding_left)*self.target_width/self.width\n",
    "                y = (y-self.height_trans+self.padding_up)*self.target_height/self.height\n",
    "\n",
    "            if x<0 or y<0:\n",
    "                coordinate = (0,0)\n",
    "                confidence = 0\n",
    "            else:\n",
    "                coordinate = (y,x)\n",
    "            \n",
    "            #print(\"coordinate: \", coordinate)\n",
    "            self.keypoint.append(coordinate)\n",
    "            self.keypoint_confidence.append(confidence)\n",
    "\n",
    "    def create_heatmap(self):\n",
    "\n",
    "        def gaussian_heatmap(image_size, center_point):\n",
    "            ro = 0\n",
    "            center_x = center_point[0]\n",
    "            center_y = center_point[1]\n",
    "\n",
    "            grid_y, grid_x = np.meshgrid(np.arange(image_size[0]), np.arange(image_size[1]))\n",
    "            x = grid_x - center_x\n",
    "            y = grid_y - center_y\n",
    "\n",
    "            return np.exp(-1 * (x ** 2 - 2 * ro * x * y + y**2) / (2 * (1 - ro**2)))\n",
    "        \n",
    "        heatmap_list = []\n",
    "        self._create_kp_list()\n",
    "        for i, kp in enumerate(self.keypoint):\n",
    "            heatmap_i = gaussian_heatmap((192, 256), kp)\n",
    "            heatmap_list.append(heatmap_i)\n",
    "\n",
    "        return heatmap_list\n",
    "\n",
    "    def show_heatmap(self, heatmap_list):\n",
    "        h = np.stack(heatmap_list)\n",
    "        plt.imshow(h.max(axis=0)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycoco = CustomCOCO(train_coco_json_path, train_coco_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycoco.load_img(1124)\n",
    "mycoco.show_original_bbox()\n",
    "#mycoco.show_crop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95999 262464 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "id_to_path = dict()\n",
    "\n",
    "mycoco = CustomCOCO(train_coco_json_path, train_coco_img_path)\n",
    "for i in range(len(mycoco)):\n",
    "    id_ = mycoco.get_id(i)\n",
    "    id_to_path[id_] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  000000185302\n",
      "before crop:  (424, 640, 3)\n",
      "X and Y -5 20 575 610\n",
      "right before cropping X and Y 20 45 600 635\n",
      "before resize:  (25, 35, 3)\n",
      "torch.Size([17, 256, 192])\n",
      "torch.Size([3, 256, 192])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f367d733760>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, hp, confidence = mycoco[id_to_path[185302]]  #1124\n",
    "# print(mycoco.target_kp)\n",
    "print(hp.shape)\n",
    "print(img.shape)\n",
    "plt.imshow(np.stack(hp).max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomCOCO(train_coco_json_path, train_coco_img_path)\n",
    "#test_data = CustomCOCO(test_coco_json_path, test_coco_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image, label, visibility) in enumerate(train_dataloader):\n",
    "    #print(\"i: \",i)\n",
    "    #plt.imshow(image)\n",
    "    if i%10 == 0:\n",
    "        print(\"i: \", i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e73d7685138b7eb2ad9427bd1ae61da544d851fb0421f6c77d829cb8307c0749"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
